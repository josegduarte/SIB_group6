{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from processing import Text_processing\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten,Embedding,Activation, Dropout, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading of Training and Test datasets defined by Full_news and Full_News_test respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\paulo\\\\Desktop\\\\Mestrado\\\\2_Ano\\\\SIB\\\\Trabalho\\\\CoAID-master\\\\CoAID-master'\n",
    "\n",
    "treated_reals_05 = pd.read_csv(path+'\\\\05-01-2020\\\\treated_reals.csv',index_col=0)\n",
    "treated_reals_05.shape\n",
    "treated_fakes_05 = pd.read_csv(path+'\\\\05-01-2020\\\\treated_fakes.csv',index_col=0)\n",
    "treated_fakes_05.shape\n",
    "treated_reals_07 = pd.read_csv(path+'\\\\07-01-2020\\\\treated_reals.csv',index_col=0)\n",
    "treated_reals_07.shape\n",
    "treated_fakes_07 = pd.read_csv(path+'\\\\07-01-2020\\\\treated_fakes.csv',index_col=0)\n",
    "treated_fakes_07.shape\n",
    "treated_reals_09 = pd.read_csv(path+'\\\\09-01-2020\\\\treated_reals.csv',index_col=0)\n",
    "treated_reals_09.shape\n",
    "treated_fakes_09 = pd.read_csv(path+'\\\\09-01-2020\\\\treated_fakes.csv',index_col=0)\n",
    "treated_fakes_09.shape\n",
    "\n",
    "Full_News = pd.concat([treated_reals_05, treated_fakes_05, treated_reals_07,\n",
    "                       treated_fakes_07])\n",
    "Full_News_test = pd.concat([treated_reals_09, treated_fakes_09])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join both Titles and Contents of News into the same column in order to maximize the amount of data that serves as an input to better train the model. This data was also treated by removing NA values and duplicates, to provide unique data to each input.\n",
    "\n",
    "The dataset is divided into two columns, one with both content and titles and one with corresponding State, that is, if the content is real the State = 1 and if the content is Fake the State = 0.\n",
    "\n",
    "In order to utilize the text of collected contents and titles it must first undergo a preprocessing routine, to exclude irrelevant words and punctuation. The preprocessing routine is defined in the processing.py module, also available in the github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_state = Full_News[['content','State']]\n",
    "titles_state = Full_News[['title','State']]\n",
    "titles_state=titles_state.rename(columns={'title':'content','State':'State'})\n",
    "\n",
    "Full = pd.concat([contents_state,titles_state],axis=0)\n",
    "Full = Full.dropna(subset = ['content'])\n",
    "Full = Full.drop_duplicates(subset = ['content'])\n",
    "\n",
    "## Apos aplicar a remocao de NA's e conteudo/titulos duplicados o valor de X baixou de 7162 para 6095\n",
    "\n",
    "text = Full['content'].tolist()\n",
    "labels = Full['State']\n",
    "\n",
    "pre_processed_text = Text_processing(text, run_all=True)\n",
    "pre_processed_text = pre_processed_text.get_processed_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of function that returns the encoded sequences, vocabulary size and Tokenizer instance, it turns each content into an array of numerical values that represent each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(pre_processed_text, max_length = 30, pad = 'post'):\n",
    "    #tokenizer to read all the words present in our corpus\n",
    "    token = Tokenizer()\n",
    "    token.fit_on_texts(pre_processed_text)\n",
    "\n",
    "    #declaring the vocab_size\n",
    "    vocab_size = len(token.word_index) + 1\n",
    "    \n",
    "    #conversion to numerical formats\n",
    "    encoded_text = token.texts_to_sequences(pre_processed_text)\n",
    "    X = pad_sequences(encoded_text, maxlen=max_length, padding=pad)\n",
    "    \n",
    "    return (X, vocab_size, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, vocab_size, token = encoding(pre_processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing a dictionary of known words used in twitter and their respective vectorized representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declaring dict to store all the words as keys in the dictionary and their vector representations as values\n",
    "glove_vectors = dict()\n",
    "\n",
    "# file = open('glove.twitter.27B.100d.txt', encoding='utf-8')\n",
    "file = open('C:\\\\Users\\\\paulo\\\\Desktop\\\\Mestrado\\\\2_Ano\\\\SIB\\\\Trabalho\\\\glove.twitter.27B\\\\glove.twitter.27B.100d.txt', encoding='utf-8')\n",
    "\n",
    "file = open('C:\\\\Users\\\\paulo\\\\Desktop\\\\Mestrado\\\\2_Ano\\\\SIB\\\\Trabalho\\\\glove.twitter.27B\\\\glove.twitter.27B.100d.txt', encoding='utf-8')\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    #storing the word in the variable\n",
    "    vectors = np.asarray(values[1: ])\n",
    "    #storing the vector representation of the respective word in the dictionary\n",
    "    glove_vectors[word] = vectors\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of a function to create a matrix for the tokens which we are present in our dataset and then storing their vector representation values in the matrix if it matches with glove_vectors words or else append the misspelled words or words which are not present to a list that is returned as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix_vec(vocab_size,dimentions):\n",
    "    misspelled = []\n",
    "    word_vector_matrix = np.zeros((vocab_size, dimentions))\n",
    "\n",
    "    for word, index in token.word_index.items():\n",
    "        vector = glove_vectors.get(word)\n",
    "        if vector is not None:\n",
    "            word_vector_matrix[index] = vector\n",
    "        else:\n",
    "            misspelled.append(word)\n",
    "            \n",
    "    return (word_vector_matrix,misspelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_matrix, misspelled_words = create_matrix_vec(vocab_size, dimentions = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       ...,\n",
       "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [ 0.016987,  0.7899  , -0.26255 , ..., -0.12681 , -0.29667 ,\n",
       "         0.31492 ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the Deep Learning model, using the Keras package and a sequencial model. We must provide the X and Y dataset as well as: \n",
    "- vocab_size = our vocabulary size\n",
    "- vec_size = the dimentions of the word vectors \n",
    "- weights = the word vector matrix \n",
    "- input_length = the maximum length of each sequence\n",
    "- trainable : As we are using glove vectors, we do not want to update the learned word weights in this model therefore this attribute is set to False.\n",
    "\n",
    "Fitting of the model: In this step the model is trained  with the training sets created at the beginning of the function, and validated with the test set(20% of the original Training dataset). This process is repeated by the number of epochs specified (30 in this case) and the model that presents the best metrics is chosen.  For this purpose we used the Sequential class of the keras package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size = 100\n",
    "def create_model(X, y, vec_size,vocab_size,max_length, word_vector_matrix,test_size=0.2, random_state=42 ):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = random_state, test_size = test_size, stratify = y )\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, vec_size, input_length=max_length, weights = [word_vector_matrix], trainable = False))\n",
    "    model.add(Conv1D(64, 8, activation = 'relu'))\n",
    "    #here 64 is number of filters and 8 is size of filters\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate = 0.0001), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs = 30, validation_data = (X_test, y_test))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 0.4749 - accuracy: 0.8337 - val_loss: 0.4455 - val_accuracy: 0.8739\n",
      "Epoch 2/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.4232 - accuracy: 0.8669 - val_loss: 0.4203 - val_accuracy: 0.8803\n",
      "Epoch 3/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.4169 - accuracy: 0.8644 - val_loss: 0.4070 - val_accuracy: 0.8803\n",
      "Epoch 4/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.4074 - accuracy: 0.8665 - val_loss: 0.3932 - val_accuracy: 0.8803\n",
      "Epoch 5/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.3814 - accuracy: 0.8730 - val_loss: 0.3847 - val_accuracy: 0.8813\n",
      "Epoch 6/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.3629 - accuracy: 0.8821 - val_loss: 0.3745 - val_accuracy: 0.8845\n",
      "Epoch 7/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.3630 - accuracy: 0.8768 - val_loss: 0.3645 - val_accuracy: 0.8855\n",
      "Epoch 8/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.3450 - accuracy: 0.8863 - val_loss: 0.3602 - val_accuracy: 0.8834\n",
      "Epoch 9/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.3441 - accuracy: 0.8808 - val_loss: 0.3453 - val_accuracy: 0.8834\n",
      "Epoch 10/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.3372 - accuracy: 0.8808 - val_loss: 0.3407 - val_accuracy: 0.8803\n",
      "Epoch 11/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.3227 - accuracy: 0.8949 - val_loss: 0.3340 - val_accuracy: 0.8876\n",
      "Epoch 12/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.3195 - accuracy: 0.8881 - val_loss: 0.3290 - val_accuracy: 0.8866\n",
      "Epoch 13/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.3186 - accuracy: 0.8894 - val_loss: 0.3184 - val_accuracy: 0.8845\n",
      "Epoch 14/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.3113 - accuracy: 0.8905 - val_loss: 0.3164 - val_accuracy: 0.8876\n",
      "Epoch 15/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.3074 - accuracy: 0.8941 - val_loss: 0.3078 - val_accuracy: 0.8887\n",
      "Epoch 16/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.2918 - accuracy: 0.9026 - val_loss: 0.3067 - val_accuracy: 0.8866\n",
      "Epoch 17/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.3101 - accuracy: 0.8905 - val_loss: 0.3000 - val_accuracy: 0.8897\n",
      "Epoch 18/30\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 0.2872 - accuracy: 0.8958 - val_loss: 0.2956 - val_accuracy: 0.8887\n",
      "Epoch 19/30\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 0.2767 - accuracy: 0.9005 - val_loss: 0.2933 - val_accuracy: 0.8887\n",
      "Epoch 20/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.2789 - accuracy: 0.9021 - val_loss: 0.2909 - val_accuracy: 0.8897\n",
      "Epoch 21/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.2578 - accuracy: 0.9075 - val_loss: 0.2893 - val_accuracy: 0.8918\n",
      "Epoch 22/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.2613 - accuracy: 0.9045 - val_loss: 0.2920 - val_accuracy: 0.8981\n",
      "Epoch 23/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.2749 - accuracy: 0.9043 - val_loss: 0.2812 - val_accuracy: 0.8908\n",
      "Epoch 24/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.2510 - accuracy: 0.9119 - val_loss: 0.2822 - val_accuracy: 0.8981\n",
      "Epoch 25/30\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 0.2440 - accuracy: 0.9106 - val_loss: 0.2779 - val_accuracy: 0.8981\n",
      "Epoch 26/30\n",
      "119/119 [==============================] - 7s 61ms/step - loss: 0.2323 - accuracy: 0.9157 - val_loss: 0.2749 - val_accuracy: 0.9002\n",
      "Epoch 27/30\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 0.2347 - accuracy: 0.9150 - val_loss: 0.2740 - val_accuracy: 0.9023\n",
      "Epoch 28/30\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 0.2500 - accuracy: 0.9086 - val_loss: 0.2766 - val_accuracy: 0.9013\n",
      "Epoch 29/30\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 0.2334 - accuracy: 0.9133 - val_loss: 0.2698 - val_accuracy: 0.9002\n",
      "Epoch 30/30\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 0.2263 - accuracy: 0.9172 - val_loss: 0.2680 - val_accuracy: 0.9013\n"
     ]
    }
   ],
   "source": [
    "max_length = 30\n",
    "modelo = create_model(X = X, y = labels ,vec_size = vec_size, vocab_size = vocab_size, max_length = max_length, word_vector_matrix = word_vector_matrix )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that provides the preproccessing, encoding and padding of a single sequence, which is necessary in case we want to predict it's class using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encode(x):\n",
    "    x = pre_processing(x)\n",
    "    x = token.texts_to_sequences(x)\n",
    "    x = pad_sequences(x, maxlen=max_length, padding='post')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Dataset preparation(remove NA's and duplicates), preprocessing and enconding, because the model only works with numerical vectors. \n",
    "Subsequently the model is evaluated utilizing this data, by predicting the classes for each sequence and comparing them to real values provided, returning the model overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_state_test = Full_News_test[['content','State']]\n",
    "titles_state_test = Full_News_test[['title','State']]\n",
    "titles_state_test = titles_state_test.rename(columns={'title':'content','State':'State'})\n",
    "\n",
    "Full_test = pd.concat([contents_state_test,titles_state_test],axis=0)\n",
    "Full_test = Full_test.dropna(subset = ['content'])\n",
    "Full_test = Full_test.drop_duplicates(subset = ['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = Full_test['content'].tolist()\n",
    "Y_test = Full_test['State']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_proc_test = Text_processing(X_test, run_all=True)\n",
    "pre_proc_test = pre_proc_test.get_processed_text()\n",
    "pre_proc_test = pre_processing(X_test)\n",
    "X_test , a, b = encoding(pre_proc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 0s 3ms/step - loss: 0.2808 - accuracy: 0.9297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28081366419792175, 0.9296987056732178]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets used for this project, corresponds to three diferent temporal periods. That being said, we used the first two time intervals to train and validate the model, and the last one to test the model's predictive capabilities, from which results an accuracy of 0.9297.\n",
    "\n",
    "This sounds like a promising result. Nevertheless we are planning to test the model once more with different datasets (from different sources), to truly evaluate its potential."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
